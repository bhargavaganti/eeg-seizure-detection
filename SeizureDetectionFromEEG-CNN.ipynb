{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.integrate import simps\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn import preprocessing\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset as torchDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "data_dir = '/home/ghn8/courses/neuroengineering/project/data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_training_data(pid):\n",
    "    ICTAL = 1\n",
    "    NONICTAL = 0\n",
    "    \n",
    "    ictal_data_dir = '/home/ghn8/courses/neuroengineering/project/data/patient_%d/ictal train/' % pid\n",
    "    nonictal_data_dir = '/home/ghn8/courses/neuroengineering/project/data/patient_%d/non-ictal train/' % pid\n",
    "    \n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for i in range(1, len(os.listdir(ictal_data_dir))+1):\n",
    "        file_path = os.path.join(ictal_data_dir, 'patient_%d_%d.mat' % (pid, i))\n",
    "        data_mat = sio.loadmat(file_path)\n",
    "        data = data_mat['data'].astype(np.float32)\n",
    "        if(np.isnan(data).any()):\n",
    "            raise(ValueError(\"NaN in ictal data\"))\n",
    "        train_inputs.append(data.T) # transpose data to get num_channels x timepoints\n",
    "        train_labels.append(ICTAL)\n",
    "    \n",
    "    for i in range(1, len(os.listdir(nonictal_data_dir))+1):\n",
    "        file_path = os.path.join(nonictal_data_dir, 'patient_%d_%d.mat' % (pid, i))\n",
    "        data_mat = sio.loadmat(file_path)\n",
    "        data = data_mat['data'].astype(np.float64)\n",
    "        if(np.isnan(data).any()):\n",
    "            raise(ValueError(\"NaN in non-ictal data\"))\n",
    "        train_inputs.append(data.T) # transpose data to get num_channels x timepoints\n",
    "        train_labels.append(NONICTAL)\n",
    "        \n",
    "    train_inputs = np.asarray(train_inputs)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    \n",
    "    return train_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_test_data(pid):\n",
    "    test_dir = '/home/ghn8/courses/neuroengineering/project/data/patient_%d/test/' % pid\n",
    "    \n",
    "    test_inputs = []\n",
    "    \n",
    "    for i in range(1, len(os.listdir(test_dir))+1):\n",
    "        file_path = os.path.join(test_dir, 'patient_%d_test_%d.mat' % (pid, i))\n",
    "        if os.path.exists(file_path):\n",
    "            data_mat = sio.loadmat(file_path)\n",
    "            data = data_mat['data'].astype(np.float32)\n",
    "            data = np.nan_to_num(data)\n",
    "            if(np.isnan(data).any()):\n",
    "                raise(ValueError(\"NaN in test data\"))\n",
    "            test_inputs.append(data.T) # transpose data to get num_channels x timepoints\n",
    "        \n",
    "    test_inputs = np.asarray(test_inputs)\n",
    "    \n",
    "    return test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PATIENTS = 7\n",
    "all_train_data = {}\n",
    "\n",
    "train_data_dump_file = os.path.join('/home/ghn8/courses/neuroengineering/project/preprocessed/all_data', 'all_train_data.pkl')\n",
    "\n",
    "if not os.path.exists(train_data_dump_file):\n",
    "    for pid in range(1, NUM_PATIENTS+1):\n",
    "        train_inputs, train_labels = get_patient_training_data(pid)\n",
    "        all_train_data['patient_%d' % pid] = { 'inputs': train_inputs, 'labels': train_labels}\n",
    "        print('Finished reading data for patient %d' % pid)\n",
    "\n",
    "    pickle.dump(all_train_data, open(train_data_dump_file, 'wb'))\n",
    "else:\n",
    "    all_train_data = pickle.load(open(train_data_dump_file, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = {}\n",
    "\n",
    "test_data_dump_file = os.path.join('/home/ghn8/courses/neuroengineering/project/preprocessed/all_data', 'all_test_data.pkl')\n",
    "\n",
    "if not os.path.exists(test_data_dump_file):\n",
    "    for pid in range(1, NUM_PATIENTS+1):\n",
    "        test_inputs = get_patient_test_data(pid)\n",
    "        all_test_data['patient_%d' % pid] = test_inputs\n",
    "        print('Finished reading test data for patient %d' % pid)\n",
    "\n",
    "    pickle.dump(all_test_data, open(test_data_dump_file, 'wb'))\n",
    "else:\n",
    "    all_test_data = pickle.load(open(test_data_dump_file, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft(timeseries):\n",
    "    return np.log10(np.absolute(np.fft.rfft(timeseries)))\n",
    "\n",
    "def get_spectral_features(timeseries):\n",
    "    raw_fft = fft(timeseries)\n",
    "    mask = np.logical_or(np.isneginf(raw_fft), np.isinf(raw_fft))\n",
    "    raw_fft[mask] = 0.0;\n",
    "\n",
    "    return raw_fft\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(features, labels):\n",
    "    assert features.shape[0] == labels.shape[0], \"Mismatched number of samples\"\n",
    "    shuffled_indices = np.random.permutation(features.shape[0])\n",
    "    \n",
    "    shuffled_features = features[shuffled_indices, :]\n",
    "    shuffled_labels = labels[shuffled_indices]\n",
    "    return shuffled_features, shuffled_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs_to_spectral_features(inputs):\n",
    "    num_samples = inputs.shape[0]\n",
    "    features = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = inputs[i, :, :].squeeze()\n",
    "\n",
    "        spectral_features = get_spectral_features(sample)\n",
    "        features.append(spectral_features)\n",
    "\n",
    "    features = np.asarray(features)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ica_clean(data, n_components):\n",
    "    ica = FastICA(n_components=n_components)\n",
    "    return ica.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_features(inputs, num_eigenvalues=-1, num_icas=0):\n",
    "    num_samples = inputs.shape[0]\n",
    "    features = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = inputs[i, :, :].squeeze()\n",
    "        \n",
    "        if num_icas > 0:\n",
    "            sample = ica_clean(sample, num_icas)\n",
    "        \n",
    "        norm_sample = preprocessing.scale(sample)\n",
    "        corr = np.corrcoef(norm_sample)\n",
    "        \n",
    "        uppper_triag_indices = np.triu_indices(corr.shape[0], 1)\n",
    "        upper_triag_vals = corr[uppper_triag_indices].ravel()\n",
    "        \n",
    "        eigenvalues, _ = np.linalg.eig(corr)\n",
    "        eigenvalues = np.sort(eigenvalues)\n",
    "        \n",
    "        if num_eigenvalues > 0:\n",
    "            eigenvalues = eigenvalues[:num_eigenvalues]\n",
    "\n",
    "        features.append(np.concatenate((upper_triag_vals, eigenvalues)))\n",
    "    features = np.asarray(features)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs_to_features(inputs):\n",
    "    spectral_features = transform_inputs_to_spectral_features(inputs, lpf_freq)\n",
    "        \n",
    "    temporal_features = get_temporal_features(inputs)\n",
    "    \n",
    "    features = np.concatenate((spectral_features, spectral_features), axis=1)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convolutional neural network\n",
    "class ConvNet_Spectral20k(nn.Module):\n",
    "    def __init__(self, num_in_channels):\n",
    "        super(ConvNet_Spectral20k, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(num_in_channels, 32, kernel_size=5),\n",
    "            # nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 16, kernel_size=5),\n",
    "            # nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5))\n",
    "        self.fc = nn.Linear(99*16, 1)\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        # print(self.num_flat_features(out))\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class ConvNet_Spectral2k(nn.Module):\n",
    "    def __init__(self, num_in_channels):\n",
    "        super(ConvNet_Spectral2k, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(num_in_channels, 32, kernel_size=5),\n",
    "            # nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 16, kernel_size=5),\n",
    "            # nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5))\n",
    "        self.fc = nn.Linear(9*16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        # print(self.num_flat_features(out))\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class EEGDataset(torchDataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features[index, :,  :].astype('float')\n",
    "        label = np.expand_dims(self.labels[index], axis=1)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "def train(train_loader, num_epochs, num_features, num_in_channels, learning_rate):\n",
    "    if num_features > 300:\n",
    "        model = ConvNet_Spectral20k(num_in_channels)\n",
    "        thresh = 5e-3\n",
    "    else:\n",
    "        model = ConvNet_Spectral2k(num_in_channels)\n",
    "        thresh = 1e-3\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Training\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss = epoch_loss / len(train_loader)\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            print(\"\\tEpoch [%d/%d], Loss: %.4f\" % (epoch+1, num_epochs, epoch_loss))\n",
    "        if epoch_loss <= thresh:\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    # Test\n",
    "    model.eval()\n",
    "    predicted_probs = np.empty((0, 1))\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "\n",
    "            probs = torch.sigmoid(model(inputs)).data.cpu().numpy()\n",
    "            predicted_probs = np.concatenate((predicted_probs, probs))\n",
    "    \n",
    "    return predicted_probs\n",
    "\n",
    "def eval(model, test_loader):\n",
    "    # Test\n",
    "    model.eval()\n",
    "    \n",
    "    all_predicted_labels = np.empty((0, 1))\n",
    "    all_probs = np.empty((0, 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            probs = torch.sigmoid(model(inputs)).data.cpu().numpy()\n",
    "            predicted_labels = np.where(probs > 0.5, 1, 0)\n",
    "\n",
    "            all_predicted_labels = np.concatenate((all_predicted_labels, predicted_labels))\n",
    "            all_probs = np.concatenate((all_probs, probs))\n",
    "    \n",
    "    return all_predicted_labels.squeeze(), all_probs.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_file = os.path.join('/home/ghn8/courses/neuroengineering/project/gia/outputs', 'cnn_spectral_2layer32-16_decay1e-4_epoch5k.csv')\n",
    "is_testing = True\n",
    "\n",
    "for pid in range(1, NUM_PATIENTS+1):\n",
    "    train_inputs = all_train_data['patient_%d' % pid]['inputs']\n",
    "    train_labels = all_train_data['patient_%d' % pid]['labels']\n",
    "    test_inputs = all_test_data['patient_%d' % pid]\n",
    "\n",
    "    train_features = transform_inputs_to_spectral_features(train_inputs)\n",
    "    test_features = transform_inputs_to_spectral_features(test_inputs)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    num_epochs = 5000\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.001\n",
    "    num_samples, num_channels, num_features = train_features.shape\n",
    "\n",
    "    \n",
    "    if is_testing:\n",
    "        print(\"Predicting patient %d\" % pid)\n",
    "        \n",
    "        if pid == 1:\n",
    "            fh = open(output_file, 'w')\n",
    "            fh.write(\"id,prediction\\n\")\n",
    "        else:\n",
    "            fh = open(output_file, 'a')\n",
    "            \n",
    "        train_dataset = EEGDataset(features=train_features,\n",
    "                                  labels=train_labels)\n",
    "        test_dataset = EEGDataset(features=test_features,\n",
    "                                  labels=np.zeros((test_features.shape[0], 1)))\n",
    "\n",
    "        # Data loader\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "\n",
    "        model = train(train_loader, num_epochs, num_features, num_channels, learning_rate)\n",
    "        predicted_probs = predict(model, test_loader)\n",
    "\n",
    "        for i in range(len(predicted_probs)):\n",
    "            fh.write(\"patient_%d_%d,%f\\n\" % (pid, i+1, predicted_probs[i]))\n",
    "        fh.close()\n",
    "    else:\n",
    "        print(\"Evaluating patient %d\" % pid)\n",
    "        val_fraction = 0.2\n",
    "\n",
    "        # creat training and validation data\n",
    "        [shuffled_features, shuffled_labels] = shuffle_data(train_features, train_labels)\n",
    "\n",
    "        num_val_samples = int(num_samples * val_fraction)\n",
    "\n",
    "        val_features = shuffled_features[:num_val_samples, :]\n",
    "        val_labels = shuffled_labels[:num_val_samples]\n",
    "\n",
    "        bootstraped_train_features = shuffled_features[num_val_samples:, :]\n",
    "        bootstraped_train_labels = shuffled_labels[num_val_samples:]\n",
    "        \n",
    "        train_dataset = EEGDataset(features=bootstraped_train_features,\n",
    "                                  labels=bootstraped_train_labels)\n",
    "        test_dataset = EEGDataset(features=val_features,\n",
    "                                  labels=val_labels)\n",
    "\n",
    "    \n",
    "        # Data loader\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "\n",
    "        model = train(train_loader, num_epochs, num_features, num_channels, learning_rate)\n",
    "        all_predicted_labels, all_probs = eval(model, test_loader)\n",
    "        accuracy = (all_predicted_labels == val_labels).sum() * 100.0 / val_labels.shape[0]\n",
    "        \n",
    "        print(\"Accuracy: %.2f\" % accuracy + \"%\")\n",
    "        print(\"AUC :\", roc_auc_score(val_labels, all_probs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
